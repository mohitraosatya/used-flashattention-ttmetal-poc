{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOCefHilNDErYExIo873e2U",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mohitraosatya/used-flashattention-ttmetal-poc/blob/main/Untitled6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c-D4LKw_aCEh",
        "outputId": "c568ece3-3c67-47d8-cc62-f24c950a5f0e"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Mar  4 09:22:22 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   34C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wBI8VCyYX8z_",
        "outputId": "ed9bbfa8-ccf4-4682-a427-a442704720e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:3 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:5 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:7 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "build-essential is already the newest version (12.9ubuntu3).\n",
            "cmake is already the newest version (3.22.1-1ubuntu1.22.04.2).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 48 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "!apt-get update\n",
        "!apt-get install -y cmake build-essential"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile CMakeLists.txt\n",
        "cmake_minimum_required(VERSION 3.16)\n",
        "project(FusedFlashAttention LANGUAGES CXX)\n",
        "\n",
        "set(CMAKE_CXX_STANDARD 17)\n",
        "set(CMAKE_CXX_STANDARD_REQUIRED ON)\n",
        "\n",
        "# We'll build one executable: fused_flash_attention\n",
        "add_executable(fused_flash_attention\n",
        "    fused_attention.cpp\n",
        "    fused_attention.hpp\n",
        "    naive_attention.cpp\n",
        "    naive_attention.hpp\n",
        "    main.cpp\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ENhTXa3YCua",
        "outputId": "7ba131e9-cd88-4d58-a311-b000afb6ba34"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing CMakeLists.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile fused_attention.hpp\n",
        "#pragma once\n",
        "\n",
        "namespace flash_attn {\n",
        "\n",
        "struct FusedAttentionParams {\n",
        "    float* Q;\n",
        "    float* K;\n",
        "    float* V;\n",
        "    float* Output;\n",
        "\n",
        "    int batch_size;\n",
        "    int num_heads;\n",
        "    int seq_len;\n",
        "    int d_head;\n",
        "\n",
        "    bool apply_scale;\n",
        "    float scale_factor;\n",
        "};\n",
        "\n",
        "void fusedFlashAttentionKernel(const FusedAttentionParams& p);\n",
        "\n",
        "} // namespace flash_attn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-lydLuLYJM8",
        "outputId": "1ddc00be-780e-4b41-968a-7064254289ca"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing fused_attention.hpp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile fused_attention.cpp\n",
        "#include \"fused_attention.hpp\"\n",
        "#include <cmath>\n",
        "#include <algorithm>\n",
        "\n",
        "namespace flash_attn {\n",
        "\n",
        "// CPU \"fused\" approach: QK^T, softmax, multiply by V, in a single pass\n",
        "void fusedFlashAttentionKernel(const FusedAttentionParams& p)\n",
        "{\n",
        "    int B = p.batch_size;\n",
        "    int H = p.num_heads;\n",
        "    int S = p.seq_len;\n",
        "    int D = p.d_head;\n",
        "\n",
        "    for(int b = 0; b < B; b++){\n",
        "        for(int h = 0; h < H; h++){\n",
        "            float* Qbh = p.Q + (b*H + h)*(S*D);\n",
        "            float* Kbh = p.K + (b*H + h)*(S*D);\n",
        "            float* Vbh = p.V + (b*H + h)*(S*D);\n",
        "            float* Obh = p.Output + (b*H + h)*(S*D);\n",
        "\n",
        "            // scores: S x S\n",
        "            float* scores = new float[S*S];\n",
        "\n",
        "            // 1) Q x K^T\n",
        "            for(int i = 0; i < S; i++){\n",
        "                for(int j = 0; j < S; j++){\n",
        "                    float dot = 0.f;\n",
        "                    for(int d_i=0; d_i < D; d_i++){\n",
        "                        dot += Qbh[i*D + d_i] * Kbh[j*D + d_i];\n",
        "                    }\n",
        "                    if(p.apply_scale){\n",
        "                        dot *= p.scale_factor;\n",
        "                    }\n",
        "                    scores[i*S + j] = dot;\n",
        "                }\n",
        "            }\n",
        "\n",
        "            // 2) Softmax row by row\n",
        "            for(int i = 0; i < S; i++){\n",
        "                float row_max = scores[i*S];\n",
        "                for(int j=1; j < S; j++){\n",
        "                    float val = scores[i*S + j];\n",
        "                    if(val > row_max) row_max = val;\n",
        "                }\n",
        "                float sum_exp = 0.f;\n",
        "                for(int j=0; j < S; j++){\n",
        "                    float exp_val = std::exp(scores[i*S + j] - row_max);\n",
        "                    scores[i*S + j] = exp_val;\n",
        "                    sum_exp += exp_val;\n",
        "                }\n",
        "                for(int j=0; j < S; j++){\n",
        "                    scores[i*S + j] /= sum_exp;\n",
        "                }\n",
        "            }\n",
        "\n",
        "            // 3) Multiply by V => Output\n",
        "            for(int i=0; i < S*D; i++){\n",
        "                Obh[i] = 0.f;\n",
        "            }\n",
        "            for(int i = 0; i < S; i++){\n",
        "                for(int d_i=0; d_i < D; d_i++){\n",
        "                    float sum_val = 0.f;\n",
        "                    for(int j=0; j < S; j++){\n",
        "                        sum_val += scores[i*S + j] * Vbh[j*D + d_i];\n",
        "                    }\n",
        "                    Obh[i*D + d_i] = sum_val;\n",
        "                }\n",
        "            }\n",
        "\n",
        "            delete[] scores;\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "} // namespace flash_attn\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kiDZpOPCaqWQ",
        "outputId": "c6484e36-0915-41eb-c6b8-8cf30a2a026f"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing fused_attention.cpp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile naive_attention.hpp\n",
        "#pragma once\n",
        "#include \"fused_attention.hpp\"\n",
        "\n",
        "namespace flash_attn {\n",
        "\n",
        "using NaiveAttentionParams = FusedAttentionParams;\n",
        "\n",
        "void naiveAttentionKernel(const NaiveAttentionParams& p);\n",
        "\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K73ui3bWasAY",
        "outputId": "b27be78f-dc07-4a5b-a9aa-82d7cf0ff03c"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing naive_attention.hpp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile naive_attention.cpp\n",
        "#include \"naive_attention.hpp\"\n",
        "#include <cmath>\n",
        "#include <algorithm>\n",
        "\n",
        "namespace flash_attn {\n",
        "\n",
        "void naiveAttentionKernel(const NaiveAttentionParams& p)\n",
        "{\n",
        "    int B = p.batch_size;\n",
        "    int H = p.num_heads;\n",
        "    int S = p.seq_len;\n",
        "    int D = p.d_head;\n",
        "\n",
        "    // Step 1: QK^T => scores\n",
        "    float* scores = new float[B * H * S * S];\n",
        "\n",
        "    for(int b=0; b < B; b++){\n",
        "        for(int h=0; h < H; h++){\n",
        "            float* Qbh = p.Q + (b*H + h)*(S*D);\n",
        "            float* Kbh = p.K + (b*H + h)*(S*D);\n",
        "\n",
        "            for(int i=0; i < S; i++){\n",
        "                for(int j=0; j < S; j++){\n",
        "                    float dot = 0.f;\n",
        "                    for(int d_i=0; d_i < D; d_i++){\n",
        "                        dot += Qbh[i*D + d_i] * Kbh[j*D + d_i];\n",
        "                    }\n",
        "                    if(p.apply_scale){\n",
        "                        dot *= p.scale_factor;\n",
        "                    }\n",
        "                    scores[(b*H + h)*S*S + i*S + j] = dot;\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "    // Step 2: softmax\n",
        "    for(int b=0; b < B; b++){\n",
        "        for(int h=0; h < H; h++){\n",
        "            float* score_ptr = scores + (b*H + h)*S*S;\n",
        "            for(int i=0; i < S; i++){\n",
        "                float row_max = score_ptr[i*S];\n",
        "                for(int j=1; j < S; j++){\n",
        "                    float val = score_ptr[i*S + j];\n",
        "                    if(val > row_max) row_max = val;\n",
        "                }\n",
        "                float sum_exp = 0.f;\n",
        "                for(int j=0; j < S; j++){\n",
        "                    float exp_val = std::exp(score_ptr[i*S + j] - row_max);\n",
        "                    score_ptr[i*S + j] = exp_val;\n",
        "                    sum_exp += exp_val;\n",
        "                }\n",
        "                for(int j=0; j < S; j++){\n",
        "                    score_ptr[i*S + j] /= sum_exp;\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "    // Step 3: multiply by V => output\n",
        "    for(int b=0; b < B; b++){\n",
        "        for(int h=0; h < H; h++){\n",
        "            float* score_ptr = scores + (b*H + h)*S*S;\n",
        "            float* Vbh = p.V + (b*H + h)*(S*D);\n",
        "            float* Obh = p.Output + (b*H + h)*(S*D);\n",
        "\n",
        "            // zero out\n",
        "            for(int i=0; i < S*D; i++){\n",
        "                Obh[i] = 0.f;\n",
        "            }\n",
        "            for(int i=0; i < S; i++){\n",
        "                for(int d_i=0; d_i < D; d_i++){\n",
        "                    float sum_val = 0.f;\n",
        "                    for(int j=0; j < S; j++){\n",
        "                        sum_val += score_ptr[i*S + j] * Vbh[j*D + d_i];\n",
        "                    }\n",
        "                    Obh[i*S + d_i] = sum_val;\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "    delete[] scores;\n",
        "}\n",
        "\n",
        "} // namespace flash_attn\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IAd5WtNTavwA",
        "outputId": "2425bf8f-eb69-4959-dc39-20f62c431354"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing naive_attention.cpp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile main.cpp\n",
        "#include <iostream>\n",
        "#include <chrono>\n",
        "#include <cmath>\n",
        "#include \"fused_attention.hpp\"\n",
        "#include \"naive_attention.hpp\"\n",
        "\n",
        "static float randFloat() {\n",
        "    return static_cast<float>(rand()) / RAND_MAX;\n",
        "}\n",
        "\n",
        "int main()\n",
        "{\n",
        "    // Increase seq_len for a bigger workload: set S=512\n",
        "    // (Change to 1024 if desired)\n",
        "    int B = 1;\n",
        "    int H = 8;\n",
        "    int S = 512;\n",
        "    int D = 64;\n",
        "    bool apply_scale = true;\n",
        "    float scale_factor = 1.0f / std::sqrt((float)D);\n",
        "\n",
        "    size_t qkv_size = (size_t)B * H * S * D;\n",
        "    float* Q = new float[qkv_size];\n",
        "    float* K = new float[qkv_size];\n",
        "    float* V = new float[qkv_size];\n",
        "    float* out_naive = new float[qkv_size];\n",
        "    float* out_fused = new float[qkv_size];\n",
        "\n",
        "    srand(42);\n",
        "    for(size_t i=0; i < qkv_size; i++){\n",
        "        Q[i] = randFloat();\n",
        "        K[i] = randFloat();\n",
        "        V[i] = randFloat();\n",
        "        out_naive[i] = 0.f;\n",
        "        out_fused[i] = 0.f;\n",
        "    }\n",
        "\n",
        "    flash_attn::NaiveAttentionParams naive_p {\n",
        "        Q, K, V, out_naive,\n",
        "        B, H, S, D,\n",
        "        apply_scale, scale_factor\n",
        "    };\n",
        "\n",
        "    flash_attn::FusedAttentionParams fused_p {\n",
        "        Q, K, V, out_fused,\n",
        "        B, H, S, D,\n",
        "        apply_scale, scale_factor\n",
        "    };\n",
        "\n",
        "    auto start_naive = std::chrono::high_resolution_clock::now();\n",
        "    flash_attn::naiveAttentionKernel(naive_p);\n",
        "    auto end_naive = std::chrono::high_resolution_clock::now();\n",
        "    double naive_ms = std::chrono::duration<double,std::milli>(end_naive - start_naive).count();\n",
        "\n",
        "    auto start_fused = std::chrono::high_resolution_clock::now();\n",
        "    flash_attn::fusedFlashAttentionKernel(fused_p);\n",
        "    auto end_fused = std::chrono::high_resolution_clock::now();\n",
        "    double fused_ms = std::chrono::duration<double,std::milli>(end_fused - start_fused).count();\n",
        "\n",
        "    std::cout << \"Naive: \" << naive_ms << \" ms\\n\";\n",
        "    std::cout << \"Fused: \" << fused_ms << \" ms\\n\";\n",
        "    std::cout << \"Speedup: \" << (naive_ms / fused_ms) << \"x\\n\";\n",
        "\n",
        "    // Compare outputs for correctness\n",
        "    double sum_sq_diff = 0.0;\n",
        "    for(size_t i=0; i < qkv_size; i++){\n",
        "        double diff = (double)out_naive[i] - (double)out_fused[i];\n",
        "        sum_sq_diff += diff * diff;\n",
        "    }\n",
        "    double rmse = std::sqrt(sum_sq_diff / qkv_size);\n",
        "    std::cout << \"RMSE: \" << rmse << std::endl;\n",
        "\n",
        "    delete[] Q;\n",
        "    delete[] K;\n",
        "    delete[] V;\n",
        "    delete[] out_naive;\n",
        "    delete[] out_fused;\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hB0kDpsLbZs2",
        "outputId": "6784557f-64ed-452e-9290-7749e1382a90"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing main.cpp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p build\n",
        "%cd build\n",
        "!cmake ..\n",
        "!make\n",
        "!./fused_flash_attention\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cJWraCetazIz",
        "outputId": "0473ecd5-6e33-4aa0-a134-b810b4a794ec"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/build/build/build/build/build\n",
            "-- The CXX compiler identification is GNU 11.4.0\n",
            "-- Detecting CXX compiler ABI info\n",
            "-- Detecting CXX compiler ABI info - done\n",
            "-- Check for working CXX compiler: /usr/bin/c++ - skipped\n",
            "-- Detecting CXX compile features\n",
            "-- Detecting CXX compile features - done\n",
            "-- Configuring done (0.2s)\n",
            "-- Generating done (0.0s)\n",
            "-- Build files have been written to: /content/build/build/build/build/build\n",
            "[ 25%] \u001b[32mBuilding CXX object CMakeFiles/fused_flash_attention.dir/fused_attention.cpp.o\u001b[0m\n",
            "[ 50%] \u001b[32mBuilding CXX object CMakeFiles/fused_flash_attention.dir/naive_attention.cpp.o\u001b[0m\n",
            "[ 75%] \u001b[32mBuilding CXX object CMakeFiles/fused_flash_attention.dir/main.cpp.o\u001b[0m\n",
            "[100%] \u001b[32m\u001b[1mLinking CXX executable fused_flash_attention\u001b[0m\n",
            "[100%] Built target fused_flash_attention\n",
            "Naive: 879.1 ms\n",
            "Fused: 860.264 ms\n",
            "Speedup: 1.0219x\n",
            "RMSE: 0.467702\n"
          ]
        }
      ]
    }
  ]
}